{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14630848,"sourceType":"datasetVersion","datasetId":9346046},{"sourceId":14778345,"sourceType":"datasetVersion","datasetId":9387219},{"sourceId":745818,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":569410,"modelId":581715}],"dockerImageVersionId":31260,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport csv\nimport time\nimport yaml\nimport shutil\nimport random\nimport numpy as np\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom pathlib import Path\nimport matplotlib.pyplot as plt \nfrom collections import Counter\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as T\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"markdown","source":"### Change this to True if you want to download into output\nhttps://www.kaggle.com/datasets/dimensi0n/imagenet-256","metadata":{}},{"cell_type":"code","source":"DOWNLOAD = False","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_yaml(path):\n    with open(path, \"r\") as f:\n        cfg = yaml.safe_load(f)\n    return cfg\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)                     # Python random\n    np.random.seed(seed)                  # NumPy\n    torch.manual_seed(seed)               # CPU\n    torch.cuda.manual_seed(seed)          # GPU\n    torch.cuda.manual_seed_all(seed)      # All GPUs\n    torch.backends.cudnn.deterministic = True  # Deterministic convs\n    torch.backends.cudnn.benchmark = False     # Disable auto-tuner for reproducibility\n    print(f\"Random seed set to {seed}\")\n\ndef save_training_plots(\n    loss_history,\n    train_acc_history,\n    test_acc_history,\n    epoch_times,\n    output_dir=\"outputs/plots\"\n):\n    output_dir = Path(output_dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n    epochs = np.arange(1, len(loss_history) + 1)\n\n    # Loss plot\n    plt.figure()\n    plt.plot(epochs, loss_history, label=\"Train Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Training Loss\")\n    plt.legend()\n    plt.grid(True)\n    plt.savefig(output_dir / \"loss.png\")\n    plt.close()\n\n    # Accuracy plot\n    plt.figure()\n    plt.plot(epochs, train_acc_history, label=\"Train Accuracy\")\n    plt.plot(epochs, test_acc_history, label=\"Test Accuracy\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.title(\"Train vs Test Accuracy\")\n    plt.legend()\n    plt.grid(True)\n    plt.savefig(output_dir / \"accuracy.png\")\n    plt.close()\n\n    # Time per epoch plot\n    plt.figure()\n    plt.plot(epochs, epoch_times, label=\"Time per Epoch (s)\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Seconds\")\n    plt.title(\"Epoch Time\")\n    plt.legend()\n    plt.grid(True)\n    plt.savefig(output_dir / \"epoch_time.png\")\n    plt.close()\n\n    print(f\"\\nPlots saved to: {output_dir.resolve()}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"VGG_CFG = load_yaml(\"/kaggle/input/vgg16-config/vgg.yaml\")\nDATA_CFG = load_yaml(\"/kaggle/input/vgg16-config/imagenet.yaml\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"def download_data(data_dir):\n    data_dir = Path(data_dir)\n    data_dir.mkdir(parents=True, exist_ok=True)\n\n    download_path = kagglehub.dataset_download(\"dimensi0n/imagenet-256\")\n\n    print(\"Path to dataset files:\", download_path)\n\n    print(\"Moving data into \", data_dir)\n    shutil.move(os.path.join(download_path, \"versions\", \"1\"), data_dir)\n    return data_dir\n\ndef process_data(download):\n    if download:\n        data_path = download_data(DATA_CFG['root'])\n    else:\n        data_path = \"/kaggle/input/imagenet-256\"\n        new_data_path = \"/kaggle/working/\"\n\n    data_path = Path(data_path)\n    new_data_path = Path(new_data_path)\n    train_path = Path(new_data_path) / \"train\"\n    test_path = Path(new_data_path) / \"test\"\n\n    train_path.mkdir(parents=True, exist_ok=True)\n    test_path.mkdir(parents=True, exist_ok=True)\n\n    class_count = 0\n    image_count = 0\n    for class_dir in tqdm(list(data_path.iterdir())):\n        if class_dir == train_path or class_dir == test_path: continue\n        if class_dir.is_dir():\n            img_paths = []\n            class_name = class_dir.name\n            class_count += 1\n            \n            for img_path in class_dir.iterdir():\n                if img_path.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]:\n                    img_paths.append(img_path)\n                    image_count += 1\n\n            random.shuffle(img_paths)\n\n            split_idx = int(len(img_paths) * DATA_CFG[\"split_ratio\"])\n            train_imgs = img_paths[:split_idx]\n            test_imgs   = img_paths[split_idx:]\n\n            (train_path / class_name).mkdir(parents=True, exist_ok=True)\n            (test_path / class_name).mkdir(parents=True, exist_ok=True)\n\n            for img in train_imgs:\n                shutil.copy(img, train_path / class_name / img.name)\n\n            for img in test_imgs:\n                shutil.copy(img, test_path / class_name / img.name)\n\n    print(f\"Class Count: {class_count} \\t Image Count: {image_count} \\t Average Image per Class: {class_count/image_count:.3f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if DOWNLOAD:\n    process_data(download=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_transforms(image_size=224, train=True):\n    if train:\n        return T.Compose([\n            T.RandomResizedCrop(image_size),\n            T.RandomHorizontalFlip(),\n            T.ToTensor(),\n            T.Normalize(mean=DATA_CFG[\"mean\"], std=DATA_CFG[\"std\"])\n        ])\n    else:\n        return T.Compose([\n            T.Resize(256),\n            T.CenterCrop(image_size),\n            T.ToTensor(),\n            T.Normalize(mean=DATA_CFG[\"mean\"], std=DATA_CFG[\"std\"])\n        ])\n        \nclass ImageNetDataset(Dataset):\n    def __init__(self, root, split=\"train\", transform=None):\n        self.root = Path(root) / split\n        self.transform = transform\n\n        # Scan class folders\n        self.classes = sorted([d.name for i, d in enumerate(self.root.iterdir()) if d.is_dir() and i % (1000 // DATA_CFG.get(\"num_classes\", 1000)) == 0])\n        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n\n        # Build list of (image_path, label)\n        self.samples = []\n        for cls_name in self.classes:\n            cls_folder = self.root / cls_name\n            for img_path in cls_folder.iterdir():\n                if img_path.suffix.lower() == \".jpg\":\n                    self.samples.append((img_path, self.class_to_idx[cls_name]))\n\n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, index):\n        img_path, label = self.samples[index]\n        image = Image.open(img_path).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n        return image, label","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class VGG16(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(VGG16, self).__init__()\n\n        self.features = nn.Sequential(\n            # Block 1\n            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n\n            # Black 2\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n\n            # Block 3\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2,2),\n\n            # Block 4\n            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2,2),\n\n            # Block 5\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2,2)\n        )\n\n        self.classifer = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(512*7*7, VGG_CFG.get(\"fc_layer\", 1024)),  # assuming input 224x224\n            nn.BatchNorm1d(VGG_CFG.get(\"fc_layer\", 1024)),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(VGG_CFG.get(\"fc_layer\", 1024), VGG_CFG.get(\"fc_layer\", 1024)),\n            nn.BatchNorm1d(VGG_CFG.get(\"fc_layer\", 1024)),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(VGG_CFG.get(\"fc_layer\", 1024), num_classes)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifer(x)\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"# Config\nset_seed(42)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\nif DOWNLOAD:\n    data_root = \"/kaggle/working/\"\nelse:\n    data_root = \"/kaggle/input/imagenet/imagenet\"\n    \n# Datasets & loaders\ntrain_dataset = ImageNetDataset(root=data_root, \n                                split=\"train\", \n                                transform=build_transforms(DATA_CFG[\"image_size\"], train=True))\ntest_dataset   = ImageNetDataset(root=data_root, \n                                split=\"test\",  \n                                transform=build_transforms(DATA_CFG[\"image_size\"], train=False))\n\ntrain_loader = DataLoader(train_dataset, \n                          batch_size=DATA_CFG[\"batch_size\"], \n                          shuffle=True, \n                          num_workers=DATA_CFG[\"num_workers\"],\n                          drop_last=True)\ntest_loader   = DataLoader(test_dataset, \n                          batch_size=DATA_CFG[\"batch_size\"], \n                          shuffle=False, \n                          num_workers=DATA_CFG[\"num_workers\"],\n                          drop_last=True)\n\n# Model, loss, optimizer\nmodel = VGG16(num_classes=DATA_CFG.get(\"num_classes\", 1000)).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(\n    model.parameters(),\n    lr=float(VGG_CFG.get(\"lr\", 0.001)),\n    momentum=float(VGG_CFG.get(\"momentum\", 0.9)),\n    weight_decay=float(VGG_CFG.get(\"weight_decay\", 1e-4)),\n)\nscheduler = optim.lr_scheduler.StepLR(\n    optimizer,\n    step_size=int(VGG_CFG.get(\"step_size\", 30)),\n    gamma=float(VGG_CFG.get(\"gamma\", 0.1)),\n)\n\n# Checkpoint\nnum_epochs = VGG_CFG.get(\"epochs\", 50)\noutput_dir = Path(\"/kaggle/working/outputs/checkpoints\")\noutput_dir.mkdir(parents=True, exist_ok=True)\n\nstart_epoch = 0\nbest_acc = 0.0\n\nloss_history = []\ntrain_acc_history = []\ntest_acc_history = []\nepoch_times = []\n\nif VGG_CFG.get(\"start_from\", None) is not None and not isinstance(VGG_CFG.get(\"start_from\", None), str):\n    ckpt_epoch = int(VGG_CFG[\"start_from\"])\n    model_dir = Path(\"/kaggle/input/vgg16/pytorch/default/1\")\n    \n    ckpt_path = model_dir / f\"vgg16_epoch_{ckpt_epoch}.pth\"\n\n    checkpoint = torch.load(ckpt_path, map_location=device)\n\n    model.load_state_dict(checkpoint[\"model_state\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n    scheduler.load_state_dict(checkpoint[\"scheduler_state\"])\n\n    best_acc = checkpoint.get(\"best_acc\", 0.0)\n\n    loss_history = checkpoint.get(\"loss_history\", [])\n    train_acc_history = checkpoint.get(\"train_acc_history\", [])\n    test_acc_history = checkpoint.get(\"test_acc_history\", [])\n    epoch_times = checkpoint.get(\"epoch_times\", [])\n\n    start_epoch = checkpoint[\"epoch\"] + 1\n\n    print(f\"Resumed from epoch {start_epoch}\")\n    \n# Training loop\nfor epoch in range(start_epoch, num_epochs+1):\n    start_time = time.time()\n    \n    # Training\n    model.train()\n    running_loss = 0.0\n    correct_train = 0\n    total_train = 0\n    for images, labels in tqdm(train_loader, desc=f\"[Train] Epoch {epoch+1}/{num_epochs}\"):\n        images, labels = images.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * images.size(0)\n        _, preds = torch.max(outputs, 1)\n        correct_train += (preds == labels).sum().item()\n        total_train += labels.size(0)\n\n    epoch_loss = running_loss / len(train_loader.dataset)\n    train_acc = correct_train / total_train\n    loss_history.append(epoch_loss)\n    train_acc_history.append(train_acc)\n\n    # Test\n    model.eval()\n    correct_test = 0\n    total_test = 0\n\n    with torch.no_grad():\n        for images, labels in tqdm(test_loader, desc=f\"[Test] Epoch {epoch+1}/{num_epochs}\"):\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, preds = torch.max(outputs, 1)\n            correct_test += (preds == labels).sum().item()\n            total_test += labels.size(0)\n\n    test_acc = correct_test / total_test\n    test_acc_history.append(test_acc)\n\n    epoch_time = time.time() - start_time\n    epoch_times.append(epoch_time)\n\n    print(f\"Epoch {epoch+1} | Loss: {epoch_loss:.4f} | Train Acc: {train_acc*100:.2f}% | Test Acc: {test_acc*100:.2f}% | Time: {epoch_time:.2f}s\")\n\n    # Save plots\n    save_training_plots(\n        loss_history=loss_history,\n        train_acc_history=train_acc_history,\n        test_acc_history=test_acc_history,\n        epoch_times=epoch_times,\n        output_dir=\"outputs/plots\"\n    )\n\n    # Save checkpoint\n    ckpt = {\n        \"epoch\": epoch,\n        \"model_state\": model.state_dict(),\n        \"optimizer_state\": optimizer.state_dict(),\n        \"scheduler_state\": scheduler.state_dict(),\n        \"best_acc\": best_acc,\n    \n        # histories\n        \"loss_history\": loss_history,\n        \"train_acc_history\": train_acc_history,\n        \"test_acc_history\": test_acc_history,\n        \"epoch_times\": epoch_times,\n    }\n    \n    ckpt_path = output_dir / f\"vgg16_epoch_{epoch}.pth\"\n    torch.save(ckpt, ckpt_path)\n    print(f\"Checkpoint saved to : {ckpt_path}\")\n    \n    if test_acc > best_acc:\n        best_acc = test_acc\n        best_ckpt_path = output_dir / \"vgg16_best.pth\"\n        torch.save(ckpt, best_ckpt_path)\n        print(f\"Saved best model to {best_ckpt_path}\")\n\n    scheduler.step()\n\nprint(\"\\nTraining Summary\")\nprint(f\"Best Test Accuracy: {best_acc*100:.2f}%\")\nprint(f\"Total time: {sum(epoch_times):.2f} seconds\")\nprint(f\"Avg time/epoch: {np.mean(epoch_times):.2f} seconds\")\nprint(f\"Min epoch time: {np.min(epoch_times):.2f} seconds\")\nprint(f\"Max epoch time: {np.max(epoch_times):.2f} seconds\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"def inference(params_path, topk=(1,5)):\n    # Setup\n    set_seed(42)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(\"Using device:\", device)\n\n    # Create output directory for plots\n    plots_dir = Path(\"/kaggle/working/outputs/plots\")\n    plots_dir.mkdir(parents=True, exist_ok=True)\n\n    # Create output directory for confusion\n    metric_dir = Path(\"/kaggle/working/outputs/metrics\")\n    metric_dir.mkdir(parents=True, exist_ok=True)\n\n    if DOWNLOAD:\n        data_root = \"/kaggle/working/\"\n    else:\n        data_root = \"/kaggle/input/imagenet/imagenet\"\n\n    # Data\n    test_dataset = ImageNetDataset(\n        root=data_root, \n        split=\"test\",  \n        transform=build_transforms(DATA_CFG[\"image_size\"], train=False)\n    )\n    test_loader = DataLoader(\n        test_dataset, batch_size=64, shuffle=False, num_workers=1\n    )\n    idx_to_class = test_dataset.classes\n\n    # Model\n    model = VGG16(num_classes=DATA_CFG.get(\"num_classes\", len(idx_to_class))).to(device)\n    ckpt_path = Path(\"/kaggle/working/outputs/checkpoints\") / params_path\n    checkpoint = torch.load(ckpt_path, map_location=device)\n    model.load_state_dict(checkpoint[\"model_state\"])\n    model.eval()\n\n    # Metrics Tracking\n    total = 0\n    topk_correct = [0] * len(topk)\n    confusion_counter = Counter()      # (true, pred)\n    per_class_total = Counter()        # true\n    per_class_correct = Counter()      # true & correct\n\n    # Inference Loop\n    with torch.no_grad():\n        for images, labels in tqdm(test_loader, desc=f\"[Inference]\"):\n            images = images.to(device, non_blocking=True)\n            labels = labels.to(device, non_blocking=True)\n\n            logits = model(images)\n            probs = torch.softmax(logits, dim=1)\n\n            # Top-k accuracy\n            for i, k in enumerate(topk):\n                topk_preds = torch.topk(probs, k, dim=1).indices\n                topk_correct[i] += (\n                    topk_preds == labels.unsqueeze(1)\n                ).any(dim=1).sum().item()\n\n            # Top-1 predictions\n            preds = torch.argmax(probs, dim=1)\n\n            for t, p in zip(labels.cpu().numpy(), preds.cpu().numpy()):\n                per_class_total[t] += 1\n                if t == p:\n                    per_class_correct[t] += 1\n                else:\n                    confusion_counter[(t, p)] += 1\n\n            total += labels.size(0)\n\n    # Print accuracy\n    print(\"\\nAccuracy:\")\n    for i, k in enumerate(topk):\n        acc = topk_correct[i] / total\n        print(f\"Top-{k}: {acc:.4f}\")\n\n    # Confusion analysis\n    most_confused = confusion_counter.most_common(10)\n\n    print(\"\\nTop 10 most confused class pairs (true -> predicted):\")\n    for (t, p), count in most_confused:\n        print(f\"{idx_to_class[t]} -> {idx_to_class[p]} : {count}\")\n\n    # Bar plot for top 10 most confused\n    if most_confused:\n        labels_plot = [\n            f\"{idx_to_class[t]}->{idx_to_class[p]}\"\n            for (t, p), _ in most_confused\n        ]\n        counts = [c for _, c in most_confused]\n\n        plt.figure(figsize=(10, 5))\n        plt.bar(range(len(counts)), counts)\n        plt.xticks(range(len(counts)), labels_plot, rotation=45)\n        plt.ylabel(\"Count\")\n        plt.title(\"Top 10 Most Confused Class Pairs\")\n        plt.tight_layout()\n\n        plot_path = plots_dir / \"most_confused_pairs.png\"\n        plt.savefig(plot_path)\n        plt.close()\n        print(f\"\\nConfusion plot saved to: {plot_path}\")\n\n    # Selected pairs for 3x4 image grid\n    selected_pairs = [\n        (\"sidewinder\", \"horned_viper\"),\n        (\"desktop_computer\", \"screen\"),\n        (\"blenheim_spaniel\", \"welsh_springer_spaniel\"),\n        (\"barn_spider\", \"wolf_spider\"),\n        (\"potpie\", \"bagel\"),\n        (\"bedlington_terrier\", \"miniature_poodle\")\n    ]\n    class_name_to_idx = {name: idx for idx, name in enumerate(idx_to_class)}\n\n    fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n    axes = axes.reshape(3, 4)  # ensure shape\n\n    for idx, (true_name, pred_name) in enumerate(selected_pairs):\n        row = idx // 2\n        col = (idx % 2) * 2  # 0 or 2\n\n        t = class_name_to_idx[true_name]\n        p = class_name_to_idx[pred_name]\n\n        # Get all images for true and predicted classes\n        t_imgs = [img_path for img_path, label in test_dataset.samples if label == t]\n        p_imgs = [img_path for img_path, label in test_dataset.samples if label == p]\n\n        # Randomly pick 2 images per class\n        t_img1, t_img2 = random.sample(t_imgs, 2)\n        p_img1, p_img2 = random.sample(p_imgs, 2)\n\n        img_list = [\n            (t_img1, true_name),\n            (p_img1, pred_name),\n            (t_img2, true_name),\n            (p_img2, pred_name)\n        ]\n\n        for i in range(2):\n            axes[row, col + i].imshow(Image.open(img_list[i][0]).convert(\"RGB\"))\n            axes[row, col + i].axis(\"off\")\n            axes[row, col + i].set_title(img_list[i][1], fontsize=10)\n\n        # Black vertical line between pair columns\n        axes[row, col + 1].spines['left'].set_color('black')\n        axes[row, col + 1].spines['left'].set_linewidth(2)\n\n    plt.tight_layout()\n    sample_img_path = plots_dir / \"most_confused_pairs_samples.png\"\n    plt.savefig(sample_img_path)\n    plt.close()\n    print(f\"\\nSample images of confused pairs saved to: {sample_img_path}\")\n\n    # Per-class accuracy CSV\n    class_accuracy = []\n    for cls in per_class_total:\n        acc = per_class_correct[cls] / per_class_total[cls]\n        class_accuracy.append(\n            (cls, idx_to_class[cls], acc, per_class_correct[cls], per_class_total[cls])\n        )\n\n    # Sort high -> low accuracy\n    class_accuracy.sort(key=lambda x: x[2], reverse=True)\n\n    csv_path = metric_dir / \"per_class_accuracy.csv\"\n    with open(csv_path, \"w\", newline=\"\") as f:\n        writer = csv.writer(f)\n        writer.writerow([\"class_id\", \"class_name\", \"accuracy\", \"correct\", \"total\"])\n        for cls, name, acc, correct, total_cls in class_accuracy:\n            writer.writerow([cls, name, f\"{acc:.4f}\", correct, total_cls])\n\n    print(f\"\\nPer-class accuracy CSV saved to: {csv_path}\")\n\nparam_path = Path(\"/kaggle/input/vgg16/pytorch/default/1\") / \"vgg16_epoch_100.pth\"\ninference(param_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def summarize_checkpoint_times(ckpt_path):\n    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n    \n    # Check if epoch_times exists\n    if \"epoch_times\" not in ckpt:\n        print(\"Checkpoint does not contain 'epoch_times'.\")\n        return None\n\n    epoch_times = ckpt[\"epoch_times\"]\n    total_time = sum(epoch_times)\n    avg_time = total_time / len(epoch_times)\n\n    def format_hms(seconds):\n        h = int(seconds // 3600)\n        m = int((seconds % 3600) // 60)\n        s = int(seconds % 60)\n        return f\"{h}h {m}m {s}s\"\n\n    print(f\"Average epoch time: {format_hms(avg_time)}\")\n    print(f\"Total training time: {format_hms(total_time)}\")\n    \n    return avg_time, total_time\n\nckpt_file = Path(\"/kaggle/input/vgg16/pytorch/default/1/vgg16_epoch_100.pth\")\nsummarize_checkpoint_times(ckpt_file)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}